{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBS_lauzhack - Entity resolution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed the given dataset and apply the proper transformations to the data in order to better anlyse it and create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#pd.set_option('precision', 3)\n",
    "display.precision = 3\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import preprocessing\n",
    "#from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import boxcox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files from the 'data' subfolder\n",
    "account_booking_df = pd.read_csv('data/account_booking_train.csv')\n",
    "external_parties_df = pd.read_csv('data/external_parties_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tables with merge using \n",
    "combined_data = pd.merge(account_booking_df, external_parties_df, on='transaction_reference_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_reference_id            0\n",
       "debit_credit_indicator              0\n",
       "account_id                          0\n",
       "transaction_amount                  0\n",
       "transaction_currency                0\n",
       "transaction_date                    0\n",
       "party_role                          0\n",
       "party_info_unstructured             0\n",
       "parsed_name                         0\n",
       "parsed_address_street_name        739\n",
       "parsed_address_street_number     3030\n",
       "parsed_address_unit             11064\n",
       "parsed_address_postal_code       3480\n",
       "parsed_address_city              1168\n",
       "parsed_address_state             9107\n",
       "parsed_address_country           6490\n",
       "party_iban                       3399\n",
       "party_phone                      5000\n",
       "external_id                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          party_phone\n",
      "0                 NaN\n",
      "1        419477655328\n",
      "2                 NaN\n",
      "3       4119006262567\n",
      "4                 NaN\n",
      "...               ...\n",
      "11059  13445497715686\n",
      "11060   4116282024224\n",
      "11061             NaN\n",
      "11062             NaN\n",
      "11063  51473539600789\n",
      "\n",
      "[11064 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Null count of each variables to know which ones are more relevant to analyze\n",
    "combined_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that have more than 50% of missing values and irrelevant information\n",
    "\n",
    "## External parties data\n",
    "\n",
    "irrelevant_cols_external = ['party_info_unstructured', 'parsed_address_unit', 'parsed_address_state', 'parsed_address_country']\n",
    "external_parties_df.drop(columns=irrelevant_cols_external, inplace=True, errors='ignore')\n",
    "\n",
    "## Accounts booking data\n",
    "duplicate_ids = account_booking_df[account_booking_df.duplicated(subset='transaction_reference_id', keep=False)]\n",
    "account_booking_df = account_booking_df[~account_booking_df['transaction_reference_id'].isin(duplicate_ids['transaction_reference_id'])]\n",
    "\n",
    "irrelevant_cols_booking = ['debit_credit_indicator']\n",
    "account_booking_df.drop(columns=irrelevant_cols_booking, inplace=True, errors='ignore')\n",
    "\n",
    "# Merge the two dataframes\n",
    "\n",
    "merged_df = pd.merge(external_parties_df, account_booking_df, on='transaction_reference_id', how='inner')\n",
    "\n",
    "merged_df.to_csv('merged_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-processing of the variable parsed_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_reference_id  parsed_name postal_code\n",
      "0                         1     John Doe       12345\n",
      "1                         2      Jon Doe       12345\n",
      "2                         3     Jane Doe       67890\n",
      "3                         4    Jonny Doe       67891\n",
      "4                         5  Sonya Sonya       67892\n"
     ]
    }
   ],
   "source": [
    "# Delete honorifics from the names\n",
    "\n",
    "# Sample data for demonstration\n",
    "merged_data_cleaned2 = pd.DataFrame({\n",
    "    'transaction_reference_id': [1, 2, 3, 4, 5],\n",
    "    'parsed_name': ['Mr. John Doe', 'Dr. Jon Doe', 'Ms. Jane Doe', 'Jonny Doe', 'Prof. Sonya Sonya'],\n",
    "    'postal_code': ['12345', '12345', '67890', '67891', '67892']\n",
    "})\n",
    "\n",
    "def delete_honorifics(name): \n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Mr. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Ms. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Mrs. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Miss ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Dr. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Prof. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Rev. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Hon. ', '')\n",
    "\n",
    "delete_honorifics(merged_data_cleaned2)\n",
    "print(merged_data_cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_reference_id  parsed_name postal_code\n",
      "0                         1     john doe       12345\n",
      "1                         2      jon doe       12345\n",
      "2                         3     jane doe       67890\n",
      "3                         4    jonny doe       67891\n",
      "4                         5  sonya sonya       67892\n"
     ]
    }
   ],
   "source": [
    "# Normalize the names\n",
    "\n",
    "def normalize_names(name):\n",
    "    name['parsed_name'] = name['parsed_name'].str.lower()\n",
    "\n",
    "normalize_names(merged_data_cleaned2)\n",
    "print(merged_data_cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_reference_id</th>\n",
       "      <th>parsed_name</th>\n",
       "      <th>postal_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>john doe</td>\n",
       "      <td>12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>jon doe</td>\n",
       "      <td>12345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>jane doe</td>\n",
       "      <td>67890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>jonny doe</td>\n",
       "      <td>67891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>sonya</td>\n",
       "      <td>67892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_reference_id parsed_name postal_code\n",
       "0                         1    john doe       12345\n",
       "1                         2     jon doe       12345\n",
       "2                         3    jane doe       67890\n",
       "3                         4   jonny doe       67891\n",
       "4                         5       sonya       67892"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_reference_id parsed_name postal_code\n",
      "0                         1    john doe       12345\n",
      "1                         2     jon doe       12345\n",
      "2                         3    jane doe       67890\n",
      "3                         4   jonny doe       67891\n",
      "4                         5       sonya       67892\n"
     ]
    }
   ],
   "source": [
    "# Delete duplicates in the names\n",
    "\n",
    "# Function to delete duplicated words in names\n",
    "def delete_duplicates(name):\n",
    "    def remove_duplicates(text):\n",
    "        words = text.split()\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word.lower() not in seen:\n",
    "                seen.add(word.lower())\n",
    "                result.append(word)\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    name['parsed_name'] = name['parsed_name'].apply(remove_duplicates)\n",
    "    return name\n",
    "\n",
    "delete_duplicates(merged_data_cleaned2) \n",
    "print(merged_data_cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from jellyfish import jaro_winkler_similarity\n",
    "import pandas as pd\n",
    "import fuzzy\n",
    "\n",
    "# Initialize Soundex or Metaphone\n",
    "metaphone = fuzzy.nysiis  # Uncomment this line to use Metaphone\n",
    "\n",
    "# Step 2: Indexing (blocking) using Soundex or Metaphone\n",
    "def create_block_key(name):\n",
    "    metaphone_code = metaphone(name)  \n",
    "    return metaphone_code\n",
    "\n",
    "# Create block keys\n",
    "#merged_data_cleaned2['block_key'] = merged_data_cleaned2.apply(lambda row: create_block_key(row['parsed_name']), axis=1)\n",
    "\n",
    "#print(merged_data_cleaned2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_reference_id parsed_name postal_code block_key\n",
      "0                         1    john doe       12345     JAN D\n",
      "1                         2     jon doe       12345     JAN D\n",
      "2                         3    jane doe       67890     JAN D\n",
      "3                         4   jonny doe       67891    JANY D\n",
      "4                         5       sonya       67892       SAN\n"
     ]
    }
   ],
   "source": [
    "def split_and_metaphone(full_name):\n",
    "    \"\"\"\n",
    "    Divide un nombre completo en sus partes (palabras), aplica Metaphone a cada parte,\n",
    "    y las junta nuevamente con espacios.\n",
    "    \"\"\"\n",
    "    if pd.isna(full_name) or not isinstance(full_name, str):  \n",
    "        return np.nan\n",
    "\n",
    "    parts = full_name.strip().split()\n",
    "\n",
    "    return ' '.join(create_block_key(part) for part in parts)\n",
    "\n",
    "# Apply the function to the 'parsed_name' column\n",
    "merged_data_cleaned2['block_key'] = merged_data_cleaned2['parsed_name'].apply(split_and_metaphone)\n",
    "print(merged_data_cleaned2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Phone pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          party_phone\n",
      "0                 NaN\n",
      "1        419477655328\n",
      "2                 NaN\n",
      "3       4119006262567\n",
      "4                 NaN\n",
      "...               ...\n",
      "11059  13445497715686\n",
      "11060   4116282024224\n",
      "11061             NaN\n",
      "11062             NaN\n",
      "11063  51473539600789\n",
      "\n",
      "[11064 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Phone number normalization\n",
    "\n",
    "combined_data['party_phone'] = combined_data['party_phone'].str.split('x').str[0]\n",
    "combined_data['party_phone'] = combined_data['party_phone'].replace(r'\\D', '', regex=True)\n",
    "combined_data['party_phone'] = combined_data['party_phone'].str.lstrip('0')\n",
    "\n",
    "data = combined_data[['party_phone']]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'transaction_reference_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/UBS_Lauzhack/usb_lauz/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'transaction_reference_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Agregar nodos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 6\u001b[0m     node_id \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_reference_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     graph\u001b[38;5;241m.\u001b[39madd_node(node_id, attributes\u001b[38;5;241m=\u001b[39mrow)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Agregar aristas basadas en similitud\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UBS_Lauzhack/usb_lauz/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Desktop/UBS_Lauzhack/usb_lauz/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Desktop/UBS_Lauzhack/usb_lauz/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'transaction_reference_id'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 3: Construcción del grafo\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Agregar nodos\n",
    "for idx, row in data.iterrows():\n",
    "    node_id = row['transaction_reference_id']\n",
    "    graph.add_node(node_id, attributes=row)\n",
    "\n",
    "# Agregar aristas basadas en similitud\n",
    "for node1, node2 in possible_pairs:  # Comparar dentro de cada bloque\n",
    "    sim_name = jaro_winkler_similarity(graph.nodes[node1]['attributes']['parsed_name'],\n",
    "                                       graph.nodes[node2]['attributes']['parsed_name'])\n",
    "    if sim_name > 0.8:\n",
    "        graph.add_edge(node1, node2)\n",
    "\n",
    "# Paso 4: Identificar componentes conexos\n",
    "clusters = list(nx.connected_components(graph))\n",
    "\n",
    "# Paso 5: Asignar IDs de cluster\n",
    "cluster_mapping = {node: cluster_id for cluster_id, cluster in enumerate(clusters) for node in cluster}\n",
    "data['external_id'] = data['transaction_reference_id'].map(cluster_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usb_lauz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
