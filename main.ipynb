{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBS_lauzhack - Entity resolution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed the given dataset and apply the proper transformations to the data in order to better anlyse it and create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#pd.set_option('precision', 3)\n",
    "display.precision = 3\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import preprocessing\n",
    "#from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import boxcox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files from the 'data' subfolder\n",
    "account_booking_df = pd.read_csv('data/account_booking_train.csv')\n",
    "external_parties_df = pd.read_csv('data/external_parties_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tables with merge using \n",
    "combined_data = pd.merge(account_booking_df, external_parties_df, on='transaction_reference_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_reference_id            0\n",
       "debit_credit_indicator              0\n",
       "account_id                          0\n",
       "transaction_amount                  0\n",
       "transaction_currency                0\n",
       "transaction_date                    0\n",
       "party_role                          0\n",
       "party_info_unstructured             0\n",
       "parsed_name                         0\n",
       "parsed_address_street_name        739\n",
       "parsed_address_street_number     3030\n",
       "parsed_address_unit             11064\n",
       "parsed_address_postal_code       3480\n",
       "parsed_address_city              1168\n",
       "parsed_address_state             9107\n",
       "parsed_address_country           6490\n",
       "party_iban                       3399\n",
       "party_phone                      5000\n",
       "external_id                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null count of each variables to know which ones are more relevant to analyze\n",
    "combined_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that have more than 50% of missing values and irrelevant information\n",
    "\n",
    "## External parties data\n",
    "\n",
    "irrelevant_cols_external = ['party_info_unstructured', 'parsed_address_unit', 'parsed_address_state', 'parsed_address_country']\n",
    "external_parties_df.drop(columns=irrelevant_cols_external, inplace=True, errors='ignore')\n",
    "\n",
    "## Accounts booking data\n",
    "duplicate_ids = account_booking_df[account_booking_df.duplicated(subset='transaction_reference_id', keep=False)]\n",
    "account_booking_df = account_booking_df[~account_booking_df['transaction_reference_id'].isin(duplicate_ids['transaction_reference_id'])]\n",
    "\n",
    "irrelevant_cols_booking = ['debit_credit_indicator']\n",
    "account_booking_df.drop(columns=irrelevant_cols_booking, inplace=True, errors='ignore')\n",
    "\n",
    "# Merge the two dataframes\n",
    "\n",
    "merged_df = pd.merge(external_parties_df, account_booking_df, on='transaction_reference_id', how='inner')\n",
    "\n",
    "merged_df.to_csv('merged_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete honorifics from the names\n",
    "\n",
    "# Sample data for demonstration\n",
    "merged_data_cleaned2 = pd.DataFrame({\n",
    "    'transaction_reference_id': [1, 2, 3],\n",
    "    'parsed_name': ['Mr. John Doe', 'Dr. Jon Doe', 'Ms. Jane Doe'],\n",
    "    'postal_code': ['12345', '12345', '67890']\n",
    "})\n",
    "\n",
    "merged_data_cleaned2['party_info_name'] = merged_data_cleaned2['party_info_name'].str.replace('Mr. ', '')\n",
    "\n",
    "print(merged_data_cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_reference_id parsed_name postal_code block_key\n",
      "0                         1    John Doe       12345      JAND\n",
      "1                         2     Jon Doe       12345      JAND\n",
      "2                         3    Jane Doe       67890     JANAD\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from jellyfish import jaro_winkler_similarity\n",
    "import pandas as pd\n",
    "import fuzzy\n",
    "\n",
    "# Initialize Soundex or Metaphone\n",
    "metaphone = fuzzy.nysiis  # Uncomment this line to use Metaphone\n",
    "\n",
    "# Sample data for demonstration\n",
    "merged_data_cleaned2 = pd.DataFrame({\n",
    "    'transaction_reference_id': [1, 2, 3],\n",
    "    'parsed_name': ['Mr. John Doe', 'Dr. Jon Doe', 'Ms. Jane Doe'],\n",
    "    'postal_code': ['12345', '12345', '67890']\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Indexing (blocking) using Soundex or Metaphone\n",
    "def create_block_key(name):\n",
    "    metaphone_code = metaphone(name)  \n",
    "    return metaphone_code\n",
    "\n",
    "# Create block keys\n",
    "merged_data_cleaned2['block_key'] = merged_data_cleaned2.apply(lambda row: create_block_key(row['parsed_name']), axis=1)\n",
    "\n",
    "print(merged_data_cleaned2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m graph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Agregar nodos\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     18\u001b[0m     node_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_reference_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     graph\u001b[38;5;241m.\u001b[39madd_node(node_id, attributes\u001b[38;5;241m=\u001b[39mrow)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Paso 1: Preprocesamiento\n",
    "def normalize_text(text):\n",
    "    return text.lower().strip().replace('.', '').replace(',', '')\n",
    "\n",
    "# Paso 2: Indexación (bloqueo)\n",
    "def create_block_key(name, postal_code):\n",
    "    return f\"{name[:4]}_{postal_code}\"\n",
    "\n",
    "# Paso 3: Construcción del grafo\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Agregar nodos\n",
    "for idx, row in data.iterrows():\n",
    "    node_id = row['transaction_reference_id']\n",
    "    graph.add_node(node_id, attributes=row)\n",
    "\n",
    "# Agregar aristas basadas en similitud\n",
    "for node1, node2 in possible_pairs:  # Comparar dentro de cada bloque\n",
    "    sim_name = jaro_winkler_similarity(graph.nodes[node1]['attributes']['parsed_name'],\n",
    "                                       graph.nodes[node2]['attributes']['parsed_name'])\n",
    "    if sim_name > 0.8:\n",
    "        graph.add_edge(node1, node2)\n",
    "\n",
    "# Paso 4: Identificar componentes conexos\n",
    "clusters = list(nx.connected_components(graph))\n",
    "\n",
    "# Paso 5: Asignar IDs de cluster\n",
    "cluster_mapping = {node: cluster_id for cluster_id, cluster in enumerate(clusters) for node in cluster}\n",
    "data['external_id'] = data['transaction_reference_id'].map(cluster_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usb_lauz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
