{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBS_lauzhack - Entity resolution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed the given dataset and apply the proper transformations to the data in order to better anlyse it and create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#pd.set_option('precision', 3)\n",
    "display.precision = 3\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import preprocessing\n",
    "#from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import boxcox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files from the 'data' subfolder\n",
    "account_booking_df = pd.read_csv('data/account_booking_train.csv')\n",
    "external_parties_df = pd.read_csv('data/external_parties_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tables with merge using \n",
    "combined_data = pd.merge(account_booking_df, external_parties_df, on='transaction_reference_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_reference_id            0\n",
       "debit_credit_indicator              0\n",
       "account_id                          0\n",
       "transaction_amount                  0\n",
       "transaction_currency                0\n",
       "transaction_date                    0\n",
       "party_role                          0\n",
       "party_info_unstructured             0\n",
       "parsed_name                         0\n",
       "parsed_address_street_name        739\n",
       "parsed_address_street_number     3030\n",
       "parsed_address_unit             11064\n",
       "parsed_address_postal_code       3480\n",
       "parsed_address_city              1168\n",
       "parsed_address_state             9107\n",
       "parsed_address_country           6490\n",
       "party_iban                       3399\n",
       "party_phone                      5000\n",
       "external_id                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null count of each variables to know which ones are more relevant to analyze\n",
    "combined_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that have more than 50% of missing values and irrelevant information\n",
    "\n",
    "## External parties data\n",
    "\n",
    "irrelevant_cols_external = ['party_info_unstructured', 'parsed_address_unit', 'parsed_address_state', 'parsed_address_country']\n",
    "external_parties_df.drop(columns=irrelevant_cols_external, inplace=True, errors='ignore')\n",
    "\n",
    "## Accounts booking data\n",
    "duplicate_ids = account_booking_df[account_booking_df.duplicated(subset='transaction_reference_id', keep=False)]\n",
    "account_booking_df = account_booking_df[~account_booking_df['transaction_reference_id'].isin(duplicate_ids['transaction_reference_id'])]\n",
    "\n",
    "irrelevant_cols_booking = ['debit_credit_indicator']\n",
    "account_booking_df.drop(columns=irrelevant_cols_booking, inplace=True, errors='ignore')\n",
    "\n",
    "# Merge the two dataframes\n",
    "\n",
    "merged_df = pd.merge(external_parties_df, account_booking_df, on='transaction_reference_id', how='inner')\n",
    "\n",
    "merged_df.to_csv('merged_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-processing of the variable parsed_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 mary mith\n",
      "1               yesneia kim\n",
      "2           w. roberson jr.\n",
      "3         azquez-nelson co.\n",
      "4             m.j. bytd iii\n",
      "                ...        \n",
      "11059    james alvarado jr.\n",
      "11060            marcnguyen\n",
      "11061          joseph davis\n",
      "11062           gonzalezltd\n",
      "11063        simmons-conway\n",
      "Name: parsed_name, Length: 11064, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Delete honorifics from the names\n",
    "\n",
    "\n",
    "def delete_honorifics(name): \n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Mr. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Ms. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Mrs. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Miss ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Dr. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Prof. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Rev. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('Hon. ', '')\n",
    "\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('mr. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('ms. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('mrs. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('miss ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('dr. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('prof. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('rev. ', '')\n",
    "    name['parsed_name'] = name['parsed_name'].str.replace('hon. ', '')\n",
    "\n",
    "delete_honorifics(merged_df)\n",
    "print(merged_df['parsed_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 mary mith\n",
      "1               yesneia kim\n",
      "2           w. roberson jr.\n",
      "3         azquez-nelson co.\n",
      "4             m.j. bytd iii\n",
      "                ...        \n",
      "11059    james alvarado jr.\n",
      "11060            marcnguyen\n",
      "11061          joseph davis\n",
      "11062           gonzalezltd\n",
      "11063        simmons-conway\n",
      "Name: parsed_name, Length: 11064, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Delete duplicates in the names\n",
    "\n",
    "# Function to delete duplicated words in names\n",
    "def delete_duplicates(name):\n",
    "    def remove_duplicates(text):\n",
    "        words = text.split()\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word.lower() not in seen:\n",
    "                seen.add(word.lower())\n",
    "                result.append(word)\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    name['parsed_name'] = name['parsed_name'].apply(remove_duplicates)\n",
    "\n",
    "delete_duplicates(merged_df) \n",
    "print(merged_df['parsed_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                MR M\n",
      "1              ASN KM\n",
      "2            RPRSN JR\n",
      "3          ASKSNLSN K\n",
      "4             MJ PT A\n",
      "5             FLRS LT\n",
      "6         KRSTFR KTRS\n",
      "7             KL KLRK\n",
      "8        FLPS ANKRPRT\n",
      "9     KR FKSR ANT TRM\n",
      "10           RPRSN JR\n",
      "11           MKL AKLR\n",
      "12             ATM KL\n",
      "13           M STFNSN\n",
      "14              SN RL\n",
      "15             NT PLK\n",
      "16               KRMN\n",
      "17            TFT ARK\n",
      "18           RMNT JNS\n",
      "19             TMSPKR\n",
      "Name: parsed_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from metaphone import doublemetaphone\n",
    "\n",
    "# Funci√≥n para aplicar Metaphone\n",
    "def apply_metaphone(value):\n",
    "    if isinstance(value, list) or isinstance(value, np.ndarray):  \n",
    "        return np.nan  \n",
    "    if pd.isna(value) or not isinstance(value, str):  \n",
    "        return np.nan\n",
    "    return doublemetaphone(value.strip().lower())[0]  \n",
    "\n",
    "\n",
    "def split_and_metaphone(full_name):\n",
    "    \"\"\"\n",
    "    Divide un nombre completo en sus partes (palabras), aplica Metaphone a cada parte,\n",
    "    y las junta nuevamente con espacios.\n",
    "    \"\"\"\n",
    "    if pd.isna(full_name) or not isinstance(full_name, str):  \n",
    "        return np.nan\n",
    "\n",
    "    parts = full_name.strip().split()\n",
    "\n",
    "    return ' '.join(apply_metaphone(part) for part in parts)\n",
    "\n",
    "# Delete irrelevant columns\n",
    "\n",
    "irrelevant_cols_external = ['party_info_unstructured', 'parsed_address_unit', 'parsed_address_state', 'parsed_address_country']\n",
    "merged_df.drop(columns=irrelevant_cols_external, inplace=True, errors='ignore')\n",
    "\n",
    "duplicate_ids = account_booking_df[account_booking_df.duplicated(subset='transaction_reference_id', keep=False)]\n",
    "merge_df = merged_df[~merged_df['transaction_reference_id'].isin(duplicate_ids['transaction_reference_id'])]\n",
    "\n",
    "irrelevant_cols_booking = ['debit_credit_indicator', 'transaction_amount', 'transaction_currency']\n",
    "merge_df.drop(columns=irrelevant_cols_booking, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# Apply Metaphone to the names and addresses\n",
    "\n",
    "merged_df['parsed_name'] = merged_df['parsed_name'].apply(split_and_metaphone)\n",
    "\n",
    "merged_df['parsed_address_street_name'] = merged_df['parsed_address_street_name'].apply(apply_metaphone)\n",
    "\n",
    "merged_df.to_csv('merged_data_cleaned2.csv', index=False)\n",
    "\n",
    "print(merged_df['parsed_name'].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Phone pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          party_phone\n",
      "0                 NaN\n",
      "1        419477655328\n",
      "2                 NaN\n",
      "3       4119006262567\n",
      "4                 NaN\n",
      "...               ...\n",
      "11059  13445497715686\n",
      "11060   4116282024224\n",
      "11061             NaN\n",
      "11062             NaN\n",
      "11063  51473539600789\n",
      "\n",
      "[11064 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Phone number normalization\n",
    "\n",
    "combined_data['party_phone'] = combined_data['party_phone'].str.split('x').str[0]\n",
    "combined_data['party_phone'] = combined_data['party_phone'].replace(r'\\D', '', regex=True)\n",
    "combined_data['party_phone'] = combined_data['party_phone'].str.lstrip('0')\n",
    "\n",
    "data = combined_data[['party_phone']]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Sample data for demonstration\n",
    "data = pd.DataFrame({\n",
    "    'transaction_reference_id': [1, 2, 3, 4, 5],\n",
    "    'parsed_name': ['John Doe', 'Jon Doe', 'Jane Doe', 'John Smith', 'Jane Smith'],\n",
    "    'postal_code': ['12345', '12345', '67890', '12345', '67890'],\n",
    "    'feature1': [10, 20, 30, 40, 50],\n",
    "    'feature2': [1, 2, 3, 4, 5],\n",
    "    'label': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['feature1', 'feature2']]\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data], early_stopping_rounds=10)\n",
    "\n",
    "# Predict\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'possible_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     graph\u001b[38;5;241m.\u001b[39madd_node(node_id, attributes\u001b[38;5;241m=\u001b[39mrow)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Agregar aristas basadas en similitud\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node1, node2 \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpossible_pairs\u001b[49m:  \u001b[38;5;66;03m# Comparar dentro de cada bloque\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     sim_name \u001b[38;5;241m=\u001b[39m jaro_winkler_similarity(graph\u001b[38;5;241m.\u001b[39mnodes[node1][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m                                        graph\u001b[38;5;241m.\u001b[39mnodes[node2][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sim_name \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'possible_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 3: Construcci√≥n del grafo\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Agregar nodos\n",
    "for idx, row in merged_data_cleaned2.iterrows():\n",
    "    node_id = row['transaction_reference_id']\n",
    "    graph.add_node(node_id, attributes=row)\n",
    "\n",
    "# Agregar aristas basadas en similitud\n",
    "for node1, node2 in possible_pairs:  # Comparar dentro de cada bloque\n",
    "    sim_name = jaro_winkler_similarity(graph.nodes[node1]['attributes']['parsed_name'],\n",
    "                                       graph.nodes[node2]['attributes']['parsed_name'])\n",
    "    if sim_name > 0.8:\n",
    "        graph.add_edge(node1, node2)\n",
    "\n",
    "# Paso 4: Identificar componentes conexos\n",
    "clusters = list(nx.connected_components(graph))\n",
    "\n",
    "# Paso 5: Asignar IDs de cluster\n",
    "cluster_mapping = {node: cluster_id for cluster_id, cluster in enumerate(clusters) for node in cluster}\n",
    "data['external_id'] = data['transaction_reference_id'].map(cluster_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usb_lauz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
