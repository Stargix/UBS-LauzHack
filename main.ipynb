{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UBS_lauzhack - Entity resolution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed the given dataset and apply the proper transformations to the data in order to better anlyse it and create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import warnings\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#pd.set_option('precision', 3)\n",
    "display.precision = 3\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import preprocessing\n",
    "#from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats import boxcox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files from the 'data' subfolder\n",
    "account_booking_df = pd.read_csv('account_booking_test.csv')\n",
    "external_parties_df = pd.read_csv('external_parties_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tables with merge using \n",
    "combined_data = pd.merge(account_booking_df, external_parties_df, on='transaction_reference_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_reference_id              0\n",
       "debit_credit_indicator                0\n",
       "account_id                            0\n",
       "transaction_amount                    0\n",
       "transaction_currency                  0\n",
       "transaction_date                      0\n",
       "party_role                            0\n",
       "party_info_unstructured               0\n",
       "parsed_name                           0\n",
       "parsed_address_street_name       105595\n",
       "parsed_address_street_number     407501\n",
       "parsed_address_unit             1481672\n",
       "parsed_address_postal_code       461156\n",
       "parsed_address_city              152702\n",
       "parsed_address_state            1222710\n",
       "parsed_address_country           832475\n",
       "party_iban                       444527\n",
       "party_phone                      666700\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null count of each variables to know which ones are more relevant to analyze\n",
    "combined_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that have more than 50% of missing values and irrelevant information\n",
    "\n",
    "## External parties data\n",
    "\n",
    "irrelevant_cols_external = ['party_info_unstructured', 'parsed_address_unit', 'parsed_address_state', 'parsed_address_country']\n",
    "external_parties_df.drop(columns=irrelevant_cols_external, inplace=True, errors='ignore')\n",
    "\n",
    "## Accounts booking data\n",
    "duplicate_ids = account_booking_df[account_booking_df.duplicated(subset='transaction_reference_id', keep=False)]\n",
    "account_booking_df = account_booking_df[~account_booking_df['transaction_reference_id'].isin(duplicate_ids['transaction_reference_id'])]\n",
    "\n",
    "irrelevant_cols_booking = ['debit_credit_indicator']\n",
    "account_booking_df.drop(columns=irrelevant_cols_booking, inplace=True, errors='ignore')\n",
    "\n",
    "# Merge the two dataframes\n",
    "\n",
    "merged_df = pd.merge(external_parties_df, account_booking_df, on='transaction_reference_id', how='inner')\n",
    "\n",
    "merged_df.to_csv('merged_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-processing of the variable parsed_name and address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       therese humphrey\n",
      "1                           holly dudley\n",
      "2                          mcfarlan dknc\n",
      "3                           jamie cooper\n",
      "4          fernandez, frost and gonzalez\n",
      "                       ...              \n",
      "1481667                   allen-gonzalez\n",
      "1481668                     mccarthh llc\n",
      "1481669                     garcia-moore\n",
      "1481670                 santiago-oconnor\n",
      "1481671                  cortez-chan co.\n",
      "Name: parsed_name, Length: 1481672, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Delete honorifics from the names\n",
    "\n",
    "honorifics = ['Mr. ', 'Ms. ', 'Mrs. ', 'Miss ', 'Dr. ', 'Prof. ', 'Rev. ', 'Hon. ', 'mr. ', 'ms. ', 'mrs. ', 'miss ', 'dr. ', 'prof. ', 'rev. ', 'hon. ']\n",
    "for honorific in honorifics:\n",
    "    merged_df['parsed_name'] = merged_df['parsed_name'].str.replace(honorific, '')\n",
    "\n",
    "\n",
    "def delete_duplicates(name):\n",
    "    def remove_duplicates(text):\n",
    "        words = text.split()\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word.lower() not in seen:\n",
    "                seen.add(word.lower())\n",
    "                result.append(word)\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    name['parsed_name'] = name['parsed_name'].apply(remove_duplicates)\n",
    "    return name\n",
    "\n",
    "merged_df = delete_duplicates(merged_df)\n",
    "\n",
    "print(merged_df['parsed_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   0RS HMFR\n",
      "1                     HL TTL\n",
      "2                MKFRLN TKNK\n",
      "3                     JM KPR\n",
      "4     FRNNTSS FRST ANT KNSLS\n",
      "5                    TLN ANK\n",
      "6         RSSNL ANT LNX LMTT\n",
      "7            KPNSLS ANK KRPP\n",
      "8                     PKKRKN\n",
      "9                 KRSTL KRFN\n",
      "10               RNT NXLS AN\n",
      "11          NRMNN RFR ANT MS\n",
      "12                   ATN HRR\n",
      "13               TNS SM0 SRR\n",
      "14                  LPSSMKSL\n",
      "15                   RTTJNSN\n",
      "16                XRLS JKRFS\n",
      "17                    TLR AT\n",
      "18                   JJ JNSN\n",
      "19                    KRJNSN\n",
      "Name: parsed_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Metaphone and split data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from metaphone import doublemetaphone\n",
    "\n",
    "# Función para aplicar Metaphone\n",
    "def apply_metaphone(value):\n",
    "    if isinstance(value, list) or isinstance(value, np.ndarray):  \n",
    "        return np.nan  \n",
    "    if pd.isna(value) or not isinstance(value, str):  \n",
    "        return np.nan\n",
    "    return doublemetaphone(value.strip().lower())[0]  \n",
    "\n",
    "\n",
    "def split_and_metaphone(full_name):\n",
    "    \"\"\"\n",
    "    Divide un nombre completo en sus partes (palabras), aplica Metaphone a cada parte,\n",
    "    y las junta nuevamente con espacios.\n",
    "    \"\"\"\n",
    "    if pd.isna(full_name) or not isinstance(full_name, str):  \n",
    "        return np.nan\n",
    "\n",
    "    parts = full_name.strip().split()\n",
    "\n",
    "    return ' '.join(apply_metaphone(part) for part in parts)\n",
    "\n",
    "# Delete irrelevant columns\n",
    "\n",
    "irrelevant_cols_external = ['party_info_unstructured', 'parsed_address_unit', 'parsed_address_state', 'parsed_address_country']\n",
    "merged_df.drop(columns=irrelevant_cols_external, inplace=True, errors='ignore')\n",
    "\n",
    "duplicate_ids = account_booking_df[account_booking_df.duplicated(subset='transaction_reference_id', keep=False)]\n",
    "merged_df = merged_df[~merged_df['transaction_reference_id'].isin(duplicate_ids['transaction_reference_id'])]\n",
    "\n",
    "irrelevant_cols_booking = ['debit_credit_indicator', 'transaction_amount', 'transaction_currency']\n",
    "merged_df.drop(columns=irrelevant_cols_booking, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# Apply Metaphone to the names and addresses\n",
    "\n",
    "merged_df['parsed_name'] = merged_df['parsed_name'].apply(split_and_metaphone)\n",
    "\n",
    "merged_df['parsed_address_street_name'] = merged_df['parsed_address_street_name'].apply(apply_metaphone)\n",
    "\n",
    "merged_df.to_csv('merged_data_cleaned2.csv', index=False)\n",
    "\n",
    "print(merged_df['parsed_name'].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Phone pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             party_phone\n",
      "0           419377046529\n",
      "1        410014355220212\n",
      "2             9479116156\n",
      "3           413433377661\n",
      "4            19238836718\n",
      "...                  ...\n",
      "1481667     418056013375\n",
      "1481668              NaN\n",
      "1481669     417972409393\n",
      "1481670       9272300615\n",
      "1481671              NaN\n",
      "\n",
      "[1481672 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Phone number normalization\n",
    "\n",
    "combined_data['party_phone'] = combined_data['party_phone'].str.split('x').str[0]\n",
    "combined_data['party_phone'] = combined_data['party_phone'].replace(r'\\D', '', regex=True)\n",
    "combined_data['party_phone'] = combined_data['party_phone'].str.lstrip('0')\n",
    "\n",
    "data = combined_data[['party_phone']]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Graf solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      account_id  counts\n",
      "9011       19011     203\n",
      "8715       18715     199\n",
      "80         10080     195\n",
      "3681       13681     195\n",
      "494        10494     193\n",
      "...          ...     ...\n",
      "7090       17090     107\n",
      "4118       14118     105\n",
      "3347       13347     105\n",
      "6898       16898     104\n",
      "1521       11521     103\n",
      "\n",
      "[10001 rows x 2 columns]\n",
      "        parsed_name  counts\n",
      "555674       555674     741\n",
      "555731       555731     728\n",
      "555465       555465     682\n",
      "555905       555905     652\n",
      "195882       195882     603\n",
      "...             ...     ...\n",
      "336851       336851       1\n",
      "336850       336850       1\n",
      "336849       336849       1\n",
      "336848       336848       1\n",
      "336863       336863       1\n",
      "\n",
      "[668530 rows x 2 columns]\n",
      "        parsed_address_street_name  counts\n",
      "620977                      620977  105595\n",
      "298190                      298190    5284\n",
      "83872                        83872    4780\n",
      "1304                          1304    2828\n",
      "396064                      396064    2630\n",
      "...                            ...     ...\n",
      "620957                      620957       1\n",
      "620958                      620958       1\n",
      "620959                      620959       1\n",
      "620960                      620960       1\n",
      "620961                      620961       1\n",
      "\n",
      "[620978 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "label_encoders = {}\n",
    "for column in merged_df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    merged_df[column] = label_encoders[column].fit_transform(merged_df[column])\n",
    "\n",
    "# Group by numbers of transactions\n",
    "\n",
    "grouped_accounts = merged_df.groupby('account_id').size().reset_index(name='counts')\n",
    "grouped_accounts = grouped_accounts.sort_values(by='counts', ascending=False)\n",
    "print(grouped_accounts)\n",
    "\n",
    "grouped_names = merged_df.groupby('parsed_name').size().reset_index(name='counts')\n",
    "grouped_names = grouped_names.sort_values(by='counts', ascending=False)\n",
    "print(grouped_names)\n",
    "\n",
    "grouped_addresses = merged_df.groupby('parsed_address_street_name').size().reset_index(name='counts')\n",
    "grouped_addresses = grouped_addresses.sort_values(by='counts', ascending=False)\n",
    "print(grouped_addresses)\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for each transaction reference id\n",
    "for idx, row in merged_df.iterrows():\n",
    "    G.add_node(row['transaction_reference_id'], account_id=row['account_id'], parsed_name=row['parsed_name'], parsed_address_street_name=row['parsed_address_street_name'])\n",
    "\n",
    "# Add edges based on grouped accounts\n",
    "for _, row in grouped_accounts.iterrows():\n",
    "    if row['counts'] >= 2:\n",
    "        account_transactions = merged_df[merged_df['account_id'] == row['account_id']]['transaction_reference_id'].tolist()\n",
    "        for i in range(len(account_transactions)):\n",
    "            for j in range(i + 1, len(account_transactions)):\n",
    "                G.add_edge(account_transactions[i], account_transactions[j], group='account')\n",
    "\n",
    "# Add edges based on grouped names\n",
    "for _, row in grouped_names.iterrows():\n",
    "    if row['counts'] >= 2:\n",
    "        name_transactions = merged_df[merged_df['parsed_name'] == row['parsed_name']]['transaction_reference_id'].tolist()\n",
    "        for i in range(len(name_transactions)):\n",
    "            for j in range(i + 1, len(name_transactions)):\n",
    "                G.add_edge(name_transactions[i], name_transactions[j], group='name')\n",
    "\n",
    "# Add edges based on grouped addresses\n",
    "for _, row in grouped_addresses.iterrows():\n",
    "    if row['counts'] >= 2 and row['parsed_address_street_name'] is not 7474:\n",
    "        address_transactions = merged_df[merged_df['parsed_address_street_name'] == row['parsed_address_street_name']]['transaction_reference_id'].tolist()\n",
    "        for i in range(len(address_transactions)):\n",
    "            for j in range(i + 1, len(address_transactions)):\n",
    "                G.add_edge(address_transactions[i], address_transactions[j], group='address')\n",
    "\n",
    "# Print the graph information\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Nodes: {list(G.nodes(data=True))[:5]}\")  # Print first 5 nodes with attributes\n",
    "print(f\"Edges: {list(G.edges(data=True))[:5]}\")  # Print first 5 edges with attributes\n",
    "\n",
    "# # Visualize the graph\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "\n",
    "# # Draw the nodes\n",
    "# nx.draw_networkx_nodes(G, pos, node_size=50)\n",
    "\n",
    "# # Draw the edges\n",
    "# nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "\n",
    "# # Draw the labels\n",
    "# nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "\n",
    "# plt.title(\"Transaction Reference ID Graph\")\n",
    "# plt.show()\n",
    "\n",
    "# Add a column 'provi_id' to the merged_df dataframe\n",
    "merged_df['provi_id'] = None\n",
    "\n",
    "# Assign 'provi_id' to nodes in each connected component\n",
    "for i, component in enumerate(nx.connected_components(G)):\n",
    "    for node in component:\n",
    "        merged_df.loc[merged_df['transaction_reference_id'] == node, 'provi_id'] = f'group_{i}'\n",
    "\n",
    "print(merged_df[['transaction_reference_id', 'provi_id']].head(20))\n",
    "\n",
    "merged_df.to_csv('works_maybe.csv', index=False)\n",
    "\n",
    "grouped_provi = merged_df.groupby('provi_id').size().reset_index(name='counts')\n",
    "grouped_provi = grouped_provi.sort_values(by='counts', ascending=False)\n",
    "\n",
    "print(grouped_provi)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the counts of provi_id\n",
    "sns.countplot(x='provi_id', data=merged_df)\n",
    "plt.title('Count Plot of provi_id')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='external_id', data=merged_df)\n",
    "plt.title('Count Plot of external_id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        50039037\n",
      "1        60044692\n",
      "2        30008244\n",
      "3        40017944\n",
      "4        40012658\n",
      "           ...   \n",
      "11059    35031814\n",
      "11060    35003780\n",
      "11061    30014262\n",
      "11062    10359417\n",
      "11063    10415954\n",
      "Name: external_id, Length: 11064, dtype: int64\n",
      "       transaction_reference_id  party_role  parsed_name  \\\n",
      "0                           220           0         3909   \n",
      "1                          2973           1          900   \n",
      "2                            38           1           24   \n",
      "3                          9927           0          897   \n",
      "4                          9036           1         3754   \n",
      "...                         ...         ...          ...   \n",
      "11059                      4960           0         1793   \n",
      "11060                      7877           0         3955   \n",
      "11061                      9307           1         2134   \n",
      "11062                      8314           1         2653   \n",
      "11063                      8477           0         5472   \n",
      "\n",
      "       parsed_address_street_name  parsed_address_street_number  \\\n",
      "0                             543                           273   \n",
      "1                            4774                          4827   \n",
      "2                            6009                          1866   \n",
      "3                            5494                          4827   \n",
      "4                            2132                          4071   \n",
      "...                           ...                           ...   \n",
      "11059                        1982                          4827   \n",
      "11060                        5350                           971   \n",
      "11061                        6861                           484   \n",
      "11062                        7473                          4827   \n",
      "11063                        6247                           378   \n",
      "\n",
      "       parsed_address_postal_code  parsed_address_city  party_iban  \\\n",
      "0                            5358                 3430        2446   \n",
      "1                            4626                 2481        4931   \n",
      "2                            5183                 5741         352   \n",
      "3                            5358                  540         837   \n",
      "4                             492                  804        4931   \n",
      "...                           ...                  ...         ...   \n",
      "11059                        5358                 4369        3592   \n",
      "11060                        5358                 4900        3204   \n",
      "11061                        4902                 5030        1802   \n",
      "11062                        4634                  643        4931   \n",
      "11063                         964                 4070        4931   \n",
      "\n",
      "       party_phone  account_id  transaction_date  provi_id  \n",
      "0             2451       22308               707         0  \n",
      "1             3586       23693               292         1  \n",
      "2             6031       23688               597         1  \n",
      "3             6031       22253               361       471  \n",
      "4             2129       23633                 7       582  \n",
      "...            ...         ...               ...       ...  \n",
      "11059          381       27460               602         1  \n",
      "11060         6031       21308               277         1  \n",
      "11061         1252       22827                84         1  \n",
      "11062         6031       28413               162         1  \n",
      "11063         3733       24696               243         1  \n",
      "\n",
      "[11064 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "# test_accounts = pd.read_csv('account_booking_test.csv')\n",
    "# test_external = pd.read_csv('external_parties_test.csv')\n",
    "\n",
    "# combined_test = pd.merge(test_accounts, test_external, on='transaction_reference_id', how='inner')\n",
    "\n",
    "# X_train = merged_df.drop(columns=['external_id'])\n",
    "# X_test = combined_test\n",
    "\n",
    "# y_train = merged_df['external_id']\n",
    "# y_test = 0\n",
    "\n",
    "print(merged_df['external_id'])\n",
    "\n",
    "# Assuming merged_df is already defined and contains the 'external_id' column\n",
    "# Convert categorical columns to numeric using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in merged_df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    merged_df[column] = label_encoders[column].fit_transform(merged_df[column])\n",
    "\n",
    "# Define features and target\n",
    "X = merged_df.drop(columns=['external_id'])\n",
    "y = merged_df['external_id']\n",
    "print(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 3\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 4, number of used features: 0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250000 -> initscore=-1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Accuracy: 0.0\n",
      "Confusion Matrix:\n",
      "[[0 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Sample data for demonstration\n",
    "data = pd.DataFrame({\n",
    "    'transaction_reference_id': [1, 2, 3, 4, 5],\n",
    "    'parsed_name': ['John Doe', 'Jon Doe', 'Jane Doe', 'John Smith', 'Jane Smith'],\n",
    "    'postal_code': ['12345', '12345', '67890', '12345', '67890'],\n",
    "    'feature1': [10, 20, 30, 40, 50],\n",
    "    'feature2': [1, 2, 3, 4, 5],\n",
    "    'label': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['feature1', 'feature2']]\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# Predict\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data_cleaned2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m graph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Agregar nodos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmerged_data_cleaned2\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      6\u001b[0m     node_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_reference_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m     graph\u001b[38;5;241m.\u001b[39madd_node(node_id, attributes\u001b[38;5;241m=\u001b[39mrow)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_data_cleaned2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paso 3: Construcción del grafo\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Agregar nodos\n",
    "for idx, row in merged_data_cleaned2.iterrows():\n",
    "    node_id = row['transaction_reference_id']\n",
    "    graph.add_node(node_id, attributes=row)\n",
    "\n",
    "# Agregar aristas basadas en similitud\n",
    "for node1, node2 in possible_pairs:  # Comparar dentro de cada bloque\n",
    "    sim_name = jaro_winkler_similarity(graph.nodes[node1]['attributes']['parsed_name'],\n",
    "                                       graph.nodes[node2]['attributes']['parsed_name'])\n",
    "    if sim_name > 0.8:\n",
    "        graph.add_edge(node1, node2)\n",
    "\n",
    "# Paso 4: Identificar componentes conexos\n",
    "clusters = list(nx.connected_components(graph))\n",
    "\n",
    "# Paso 5: Asignar IDs de cluster\n",
    "cluster_mapping = {node: cluster_id for cluster_id, cluster in enumerate(clusters) for node in cluster}\n",
    "data['external_id'] = data['transaction_reference_id'].map(cluster_mapping)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usb_lauz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
